version: '3'
services:
  evaluator:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app  # Mount entire project
      - ./data:/data/continual  # Mount data directory
      - ./metrics:/app/metrics  # For metrics input
      - ./plots:/app/plots    # For evaluation plots
      - ./logs:/app/logs    # For training logs
    container_name: countdown-evaluator
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0  # Only need one GPU for evaluation
      - METRICS_DIR=/app/metrics
      - PLOTS_DIR=/app/plots
      - LOGS_DIR=/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: '8gb'  # Reduced shared memory since we only need one GPU
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    working_dir: /app
    command: >
      bash -c '
        git config --global --add safe.directory /app &&
        . /opt/conda/etc/profile.d/conda.sh &&
        conda activate zero &&
        mkdir -p /app/metrics /app/plots /app/logs &&
        # First evaluate 0.5B model
        conda run -n zero bash scripts/eval_continual_countdown.sh --model-size 0.5b &&
        # Then evaluate 1.5B model
        conda run -n zero bash scripts/eval_continual_countdown.sh --model-size 1.5b
      '

  data-generator:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app  # Mount entire project
      - ./data:/data/continual  # Mount data directory
    container_name: countdown-data-generator
    working_dir: /app
    command: >
      bash -c '
        git config --global --add safe.directory /app &&
        . /opt/conda/etc/profile.d/conda.sh &&
        conda activate zero &&
        mkdir -p /data/continual &&
        chmod -R 777 /data/continual &&
        python experiments/continual/data_gen_efficient.py
      '

  curriculum-trainer-1.5b:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app  # Mount entire project
      #- ./data:/data/continual  # Mount data directory
      #- ./wandb:/app/wandb  # For WandB logs
      #- ./logs:/app/logs    # For training logs
      - ~/model/qwen1.5b:/app/models/qwen1.5b:ro  # Mount Qwen model as read-only
      #- ./metrics:/app/metrics  # For metrics output
      #- ./checkpoints:/app/checkpoints  # Mount checkpoints directory
    container_name: countdown-curriculum-trainer-1.5b
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - BASE_MODEL=/app/models/qwen1.5b  # Path to mounted Qwen model
      - N_GPUS=8  # Using all 8 GPUs
      - ROLLOUT_TP_SIZE=2  # Tensor parallel size
      #- DATA_DIR=/data/continual  # Match the mounted data path
      - WANDB_MODE=offline  # Run WandB in offline mode
      - VLLM_ATTENTION_BACKEND=XFORMERS
      - CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
      - NCCL_DEBUG=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 8
              capabilities: [gpu]
    shm_size: '64gb'
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    working_dir: /app
    command: >
      bash -c '
        git config --global --add safe.directory /app &&
        . /opt/conda/etc/profile.d/conda.sh &&
        conda activate zero &&
        bash scripts/train_continual_countdown_1.5b_curriculum.sh
      '

  curriculum-trainer-3b:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app  # Mount entire project
      #- ./data:/data/continual  # Mount data directory
      #- ./wandb:/app/wandb  # For WandB logs
      #- ./logs:/app/logs    # For training logs
      - ~/model/qwen3b:/app/models/qwen3b:ro  # Mount Qwen model as read-only
      #- ./metrics:/app/metrics  # For metrics output
      #- ./checkpoints:/app/checkpoints  # Mount checkpoints directory
    container_name: countdown-curriculum-trainer-3b
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - BASE_MODEL=/app/models/qwen3b  # Path to mounted Qwen model
      - N_GPUS=4  # Using 4 A800 GPUs
      - ROLLOUT_TP_SIZE=2  # Tensor parallel size optimized for 4 GPUs
      #- DATA_DIR=/data/continual  # Match the mounted data path
      - WANDB_MODE=offline  # Run WandB in offline mode
      - VLLM_ATTENTION_BACKEND=XFORMERS
      - CUDA_VISIBLE_DEVICES=0,1,2,3
      - NCCL_DEBUG=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
    shm_size: '128gb'  # Increased for larger model
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    working_dir: /app
    command: >
      bash -c '
        git config --global --add safe.directory /app &&
        . /opt/conda/etc/profile.d/conda.sh &&
        conda activate zero &&
        bash scripts/train_continual_countdown_3b_curriculum.sh
      '
